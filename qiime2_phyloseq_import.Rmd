---
title: "qiime2_phyloseq_import"
author: "Diana_Gutierrez"
date: "8/20/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
To load the Bioconductor package follow the instructions in the following website: 
https://cran.r-project.org/web/packages/BiocManager/vignettes/BiocManager.html

##Environment inititation

We will begin by customizing our global settings, activating packages and loading our data into R using the following steps:

1) Set global knitr options
2) Load libraries (the app store of R)
3) Set global ggplot2 theme and options
4) Load data


### Set global knitr options

Knitr is a standardized library which "knits" together code chunks and converts them to specified format such as HTML or PDF. This is very useful for report generation. The way in which knitr handles chunk formatting and report generation can be specified in a code chunk. 

There are a large number of ways to customize R code chunks. For the knitr and ggplot2 theme settings (below) I have decided to set include=FALSE (e.g. {r global_options, include=FALSE}). This tells knitr to exclude the chunk from the final report. In this case, the chunk will still be evaluated as part of the RMarkdown document. If you wish to prevent the chunk from being executed at all you can set eval=FALSE.There are a number options you can use in this section [read about here](https://yihui.name/knitr/options/).

```{r global_options, include=FALSE}
# This chunk defines output figure dimensions,
# specifies a path where knitted figures will reside after knitting, 
# and prevents display of warnings in the knitted report
knitr::opts_chunk$set(fig.width=8,
                      fig.height=6,
                      fig.path="../figures/",
                      dev='png',
                      warning=FALSE,
                      message=FALSE)
```


You have to install BiocManager to be able to install the libraries that qiime2R depends on
```{r}
chooseCRANmirror()
install.packages("BiocManager")

#To install dependencies from the BiocManager you can use the following command: 

BiocManager::install(c("Biostrings", "biomformat", "phyloseq"))
BiocManager::install(c("DESeq2"))
BiocManager::install(c("microbiome"))
```


To install qiime2R you can reffer to this tutorial
https://forum.qiime2.org/t/tutorial-integrating-qiime2-and-r-for-data-visualization-and-analysis-using-qiime2r/4121


or use the following command:
if (!requireNamespace("devtools", quietly = TRUE)){install.packages("devtools")}
devtools::install_github("jbisanz/qiime2R") # current version is 0.99.20

## Load libraries

```{r initiate-environment}
# Each line below will load a R library and print its currently installed version
# Notes may be displayed as the packages load, but for the most part these can be ignored
install.packages("ape")
install.packages("Hmisc")
installed.packages("yaml")
install.packages("tidyr")
installed.packages("dplyr")
install.packages("DESeq2")

library("plyr"); packageVersion("plyr")
library("tidyverse"); packageVersion("tidyverse")
library("phyloseq"); packageVersion("phyloseq")
library("vegan"); packageVersion("vegan")
library("gridExtra"); packageVersion("gridExtra")
library("knitr"); packageVersion("knitr")
library("DESeq2"); packageVersion("DESeq2")
library("plotly"); packageVersion("plotly")
library("microbiome"); packageVersion("microbiome")
library("ggpubr"); packageVersion("ggpubr")
library("data.table"); packageVersion("data.table")

#All other dependencies are included in the following libraries 


library(knitr)
library(qiime2R)
library(ape)
library(Biostrings)
library(biomformat)
library(phyloseq)
library(cowplot)
library(Hmisc)
library(yaml)
library(tidyr)
library(dplyr)

```

## Set global ggplot2 theme and options

This sets the plotting aesthetics for every ggplot2 for the rest of the document. There are a tremendous number of ways to customize your ggplot2 settings using theme_set (see: http://ggplot2.tidyverse.org/reference/theme_get.html). It is best practice to do this at the beginning of the RMarkdown document so that these settings propagated to every plot through the rest of the document.

```{r global-theme-settings, include=FALSE}
# Set global theming
# This theme set will change the ggplot2 defaults to use the b&w settings (removes the default gray background) and sets the default font to 12pt Arial
theme_set(theme_bw(base_size = 12))
```
## Read in your data

Since we are working from running the DADA2 program on our samples in Qiime, first we load the table.gza object to the read_qza function to convert it into a phyloseq object 

```{r}
#I am setting the working directory to the location of the table.qza object I want to transform

list.files()
#To see details on how the main functions stores the qiime2 artifact you can run: 
?read_qza

#Details on how the data is read into the object can be ween in the help window on your  right --->

SVs<-read_qza("q2_artfcts/table.qza")

names(SVs)

#read the metadata 

metadata<-read_q2metadata("metadata/metadata_cat.tsv")
head(metadata)
type(metadata)
taxonomy<-read_qza("q2_artfcts/taxonomy.qza")
head(taxonomy$data)

#When the taxonomy artifact is imported it is imported as a string so we need to parse it to a table so we can use in downstream analysis

taxonomy<-parse_taxonomy(taxonomy$data)
head(taxonomy)

```


##Generatoin of the phyloseq object 

Now we generate the phyloseq object, by reading each one of the artifacts exported from qiime using the qza_to_phyloseq function. Note that the metada file has to have the file format including #q2:types comments in the second row of the file in order for the function to work properly and it does not accept missing values in the file. 

```{r}

ps<-qza_to_phyloseq(
    features="q2_artfcts/table.qza",
    tree="q2_artfcts/rooted-tree.qza","q2_artfcts/taxonomy.qza",
    metadata = "metadata/metadata_cat.tsv"
    )
ps

```

Here we can see that we have a phyloseq object that consists of: 
- An OTU (in this case contains Amplicon Sequence Variants [ASVs]) table with 6738 taxa and 279 samples (this includes the baseline and week 13 samples)
- A sample data file consisting of 11 variables
- A Taxonomy table with 7 taxonomic ranks
- A phylogenetic tree with 6738 tips and 6722 internal nodes

##What does the phyloseq object contain?

Lets looks at the data in more detail: 
```{r}
nsamples(ps)
```
We have 279 samples

```{r}
sample_names(ps)
```
The sample names 

```{r}
sample_variables(ps)
```
The metadata for the phyloseq object contains the following variable names

```{r}
head(sample_data(ps))
```
To look at the data from one of those variables we can use 
```{r}
sample_data(ps)$animal_id
#Because the data is considered a Factor by R it displays it in alphabetical order instead of numerical
```

We can display a summary table of the variables we have in the metadata 
```{r}
table(sample_data(ps)$diet_group)
table(sample_data(ps)$sex)
table(sample_data(ps)$zt_time)
```
```{r}

mtdt<- data.frame(sample_data(ps))
head(mtdt)

```
Specific components of the ps object can be extracted and converted to a data.frame for additional analyses.

####Factor reordering and renaming

Before we continue with any analysis we need to reorder and rename variables to make things more convenient for us in downstream analysis. The default sorting for ggplot2 is alphabetical. For example, if you want to make a box plot comparing Shannon diversity between MLF and MHF mice, it will by default always place knockout on the MHF and MLF on the right. However, you may wish to switch this order.

This can be done on a plot-by-plot basis, however, it is likely that you will want all of your plots to reflect this customization throughout 
the entire analysis, so it is useful to have an R chunk at the very beginning of your workflow to specify order and label names.

In the example data, most of the analysis will be done comparing the sample variable "diet_group" which is either MHF, MLF, EHF or ELF included in the mapping file. 

```{r factor-adjustments}
# Reorder Diet Groups
levels(sample_data(ps)$diet_group)
sample_data(ps)$diet_group <- factor(sample_data(ps)$diet_group, levels = c("MHF","MLF","EHF","LHF"))
levels(sample_data(ps)$diet_group)

# Reorder Time points
levels(sample_data(ps)$zt_time)
sample_data(ps)$zt_time <- factor(sample_data(ps)$zt_time, levels = c("0", "4", "8", "12", "16", "20"))
levels(sample_data(ps)$zt_time)

# Reorder Time points
levels(sample_data(ps)$animal_id)
sample_data(ps)$animal_id <- factor(sample_data(ps)$animal_id, levels = c("1", "2", "3", "4","5","6","7","8", "9","10","11", "12","13","14", "16","17","18","19", "20", "21", "22", "23", "24", "25", "26", "27", "28", "29", "30", "31", "32"))
levels(sample_data(ps)$animal_id)

```

#Eliminate samples from different timepoints

Our samples include several weeks of the study and we may want to filter out samples by an specific variable, in this case we will only analyze the data at week 13 of the study so we will filter all samples where the variable week_name is "Baseline"

```{r}
levels(sample_data(ps)$week_name)
ps<-subset_samples(ps, week_name != "Baseline")
levels(sample_data(ps)$week_name)

#We also want to exclude from the analysis any animals that had irregular behavior and we have already excluded from the statistical analysis of the metabolic data.

levels(sample_data(ps)$animal_id)
ps<-subset_samples(ps, animal_id != "13")
levels(sample_data(ps)$animal_id)

```

Reviewing the content of the phyloseq object we can see

```{r}
ps
```
Here we can see that now we have a phyloseq object that consists of: 
- An OTU (in this case contains Amplicon Sequence Variants [ASVs]) table with still 6738 taxa and but now we only have 178 samples (week 13)
- A sample data file consisting of 11 variables
- A Taxonomy table with 7 taxonomic ranks
- A phylogenetic tree with 6738 tips and 6722 internal nodes

The data file still has the same number of variables, and the taxonomy table has the same 7 taxonomic ranks, this makes sense, but we should have lost some of the tree branches since we lost some of the samples in the study. 

First lets check how many total taxa we have 

```{r}
summary(taxa_sums(ps))
```

We will now remove the taxa no longer part of the count table due to sample removal
```{r}
ps<-prune_taxa(taxa_sums(ps)>0, ps)
summary(taxa_sums(ps))
```
```{r}
ps
```
Here we can see that now we have a phyloseq object that consists of: 
- An OTU (in this case contains Amplicon Sequence Variants [ASVs]) table with now 3938 taxa and 178 samples (week 13)
- A sample data file consisting of 11 variables
- A Taxonomy table with 7 taxonomic ranks
- A now our phylogenetic tree contains only 3938 tips and 3933 internal nodes

####Examining read and taxa characteristics 


```{r}
sort(table(tax_table(ps) [, "Phylum"], exclude=NULL))
```
In our dataset we have a total of 21 categories at the Phylum level. As expected the large majority of taxa belong to the Firmicutes and Bacteroidetes phyla and comming right behind is the Proteobacteria phylum.  

The phyla where only one feature was observed, may be worth filtering, in this case they include Crenarcheota, SR1, Chlamydiae, Synergistetes, WPS-2, Chlorobi, GN02. The phyla that are ambiguously identified "<NA>" may need to be eliminated. 

```{r}
sort(table(tax_table(ps) [, "Kingdom"], exclude=NULL))
```
There is also contamination from non bacterial phyla like Chloroflexi and we should eliminate those as well. 

##Examining the number of reads for each sample

```{r}
sort(sample_sums(ps))

```
This list does not make a lot of sense, so we can graph it in a histogram 

```{r}
hist(sample_sums(ps), main="Histogram: Read Counts", xlab="Total Reads", 
     border="blue", col="green", las=1, breaks=12)
```
We can add this information to the phyloseq metadata 
```{r}
sample_data(ps)$total_reads <- sample_sums(ps)
head(sample_data(ps))
#mean(sample_data(ps)$total_reads)
#median(sample_data(ps)$total_reads)
```
###Examinins the OTU/AVS table 
```{r}
ntaxa(ps)

```

```{r}
head(taxa_names(ps)) #this command just shows the first taxa names
asv_tab <- data.frame(otu_table(ps)) #this command saves the AVS sable as a data frame 

```



###Examining the taxonomy
```{r}
rank_names(ps)
```
We have seven taxonomical levels 

```{r}
head(tax_table(ps))
sort(table(tax_table(ps)[, 2]))

```
This evaluation can help you identified errors in sequecing that made through the denoising step. 

####Continuation to filtering... 

We observed before that many taxa are not clearly annotated at the phylum level. Now we can remove samples that have ambiguous phylum annotation 

```{r}
summary(taxa_sums(ps))
sort(table(tax_table(ps) [, "Phylum"]))
ps0<-subset_taxa(ps, !is.na(Phylum) & !Phylum %in% c("", "uncharacterized") ) #this command subsets the phyloseq object eliminating the ambigously annotated phylaa                                                                     
summary(taxa_sums(ps0)) #lets see how the summary of them has changed
sort(table(tax_table(ps0) [, "Phylum"], exclude=NULL))  #now you see we no longer have 
```




##Investigating low prevalence/Abundance phylum and subset them out 

```{r}
#First we generate a prevalence table
prevelancedf = apply(X = otu_table(ps0),
                 MARGIN = 1,
                 FUN = function(x){sum(x > 0)})
# Add taxonomy and total read counts to this data.frame
prevelancedf = data.frame(Prevalence = prevelancedf,
                      TotalAbundance = taxa_sums(ps0),
                      tax_table(ps0))
prevelancedf[1:10,]


plyr::ddply(prevelancedf, "Phylum", function(df1){
  data.frame(mean_prevalence=mean(df1$Prevalence),sum_prevalence=sum(df1$Prevalence),total_abundance=sum(df1$TotalAbundance,na.rm = T),stringsAsFactors = F)})
```


```{r}


# Compute prevalence of each feature, store as data.frame
prevdf = apply(X = otu_table(ps0),
                 MARGIN = ifelse(taxa_are_rows(ps0), yes = 1, no = 2),
                 FUN = function(x){sum(x > 0)})
# Add taxonomy and total read counts to this data.frame
prevdf = data.frame(Prevalence = prevdf,
                      TotalAbundance = taxa_sums(ps0),
                      tax_table(ps0))

plyr::ddply(prevdf, "Phylum", function(df1){cbind(mean(df1$Prevalence),sum(df1$Prevalence))})
```
```{r}

ps0


```


#We will filter the samples with very low frequency reads (>5) 

```{r}
phyla2filter = c("Acidobacteria", "Armatimonadetes", "Chlamydiae", "Gemmatimonadetes", "GN02", "Lentisphaerae", "OD1","Planctomycetes")

#Filter entries with phylums included in the list we made
ps0=subset_taxa(ps0, !Phylum %in% phyla2filter)


ps0

hist(sample_sums(ps), main="Histogram: Read Counts", xlab="Total Reads", 
     border="blue", col="green", las=1, breaks=12)
```

```{r}
metadata$total_reads <- sample_sums(ps)
```











## Data summary and assessment

While there are numerous possible ways to evaluate your data, a standard starting approach would consist of the following steps:

1) Evaluate Amplicon Sequence Variants (ASV) summary statistics
2) Detect and remove outlier samples
3) Taxon cleaning
4) Prevalence estimation and filtering

*Step 1: Evaluate Amplicon Sequence Variants (ASV) summary statistics*
Begin by running the following R chunk to produce ASV summary plots

```{r data-assessment}
# Create a new data frame of the sorted row sums, a column of sorted values from 1 to the total number of individuals/counts for each ASV and a categorical variable stating these are all ASVs.
readsumsdf <- data.frame(nreads = sort(taxa_sums(ps0), decreasing = TRUE), 
                        sorted = 1:ntaxa(ps0),
                        type = "ASVs")
# Make a data frame with a column for the read counts of each sample for histogram production
sample_sum_df <- data.frame(sum = sample_sums(ps0))
# Make plots
# Generates a bar plot with # of reads (y-axis) for each taxa. Sorted from most to least abundant
# Generates a second bar plot with # of reads (y-axis) per sample. Sorted from most to least
p.reads = ggplot(readsumsdf, aes(x = sorted, y = nreads)) +
  geom_bar(stat = "identity") +
  ggtitle("ASV Assessment") +
  scale_y_log10() +
  facet_wrap(~type, scales = "free") +
  ylab("# of Sequences")
# Histogram of the number of Samples (y-axis) at various read depths
p.reads.hist <- ggplot(sample_sum_df, aes(x = sum)) + 
  geom_histogram(color = "black", fill = "firebrick3", binwidth = 150) +
  ggtitle("Distribution of sample sequencing depth") + 
  xlab("Read counts") +
  ylab("# of Samples")
# Final plot, side-by-side
grid.arrange(p.reads, ncol = 1)
grid.arrange( p.reads.hist, ncol=1)
# Basic summary statistics
summary(sample_sums(ps0))
```

The above data assessment is useful for getting an idea of 
1) the number of sequences per taxa (first plot). This will normally be a "long tail" with some taxa being highly abundant in the data tapering off to taxa with very few reads, 

2) the number of reads per sample (second table). Note the spike at the lowest number of reads due to samples taken from mice given antibiotics. Very low read count can be indicative of a failed reaction. Both of these plots will help give an understanding of how your data are structured across taxa and samples and will vary depending on the nature of your samples.

Samples with unexpectedly low number of sequences can be considered for removal. This is an intuitive process and should be instructed by your understanding of the samples in your study. For example, if you have 5 samples from stool samples, one would expect to obtain thousands, if not several thousands of ASV. This may not be the case for other tissues, such as spinal fluid or tissue samples. Similarly, you may not expect thousands of ASV from samples obtained from antibiotic treated organisms. Following antibiotic treatment you may be left with only dozens or hundreds of ASV. So contextual awareness about the biology of your system should guide your decision to remove samples based on ASV number.

Importantly, at each stage you should document and justify your decisions. If you are concerned that sample removal will alter the interpretation of your results, you should run your analysis on the full data and the data with the sample(s) removed to see how the decision affects your interpretation.

The above plots provide overall summaries about the number of ASV found in all of your samples. However, they are not very useful for identifying and removing specific samples. One way to do this is using code from the following R chunk.

*Step 2: Detect and remove outlier samples*
Detecting and potentially removing samples outliers (those samples with underlying data that do not conform to experimental or biological expectations) can be useful for minimizing technical variance. One way to identify sample outliers is shown in the R chunk below.

```{r sample-removal-identification}
# Format a data table to combine sample summary data with sample variable data
ss <- sample_sums(ps0)
head(ss)
sd <- as.data.frame(sample_data(ps0))
head(sd)
ss.df <- merge(sd, data.frame("ASV" = ss), by ="row.names")
head(ss.df)
# Plot the data by the treatment variable
y = 30000 # Set a threshold for the minimum number of acceptable reads. Can start as a guess
x = "sex" # Set the x-axis variable you want to examine
label = "animal_id" # This is the label you want to overlay on the points
p.ss.boxplot <- ggplot(ss.df, aes_string(x, y = "ASV", color = "sex")) + 
  geom_boxplot(outlier.colour="NA", position = position_dodge(width = 0.8)) +
  geom_jitter(size = 2, alpha = 0.6) +
  scale_y_log10() +
  facet_wrap(~zt_time) +
  geom_hline(yintercept = y, lty = 2) +
  geom_text(aes_string(label = label), size = 5, nudge_y = 0.05, nudge_x = 0.05)
p.ss.boxplot
```
The example data does hot have many samples with fewer than 3,000 ASV other than the sample 5 in the ZT 12 graph. When questionable samples arise you should take note of them so if there are samples which behave oddly in downstream analysis you can recall this information and perhaps justify their removal. In this case lets remove them for practice. 

```{r sample-outlier-removal}
nsamples(ps0)

#levels(sample_data(ps0)$name)
ps1 <- subset_samples(ps0, name !="5x3x12")

nsamples(ps1)
```

Note that we created a new PhyloSeq object called ps1. This preserves all of the data in the original ps0 and creates a new data object with the offending samples removed called ps1.

Failure to detect and remove "bad" samples can make interpreting ordinations much more challenging as they typically project as "outliers" severely skewing the rest of the samples. These samples also increase variance and will impede your ability to identify diferentially abundant taxa between groups. Thus sample outlier removal should be a serious and thoughtful part of every analysis in order to obtain optimal results.

###Alpha-Diversity 

```{r}
head(estimate_richness(ps1))
```

```{r}

(p <- plot_richness(ps, x = "zt_time", color = "sex", measures = c("Observed", "Shannon")))

p + labs(x = "", y = "Alpha Diversity Measure\n") + 
  theme_bw()


```
###Beta diversity ordination 

```{r}
ps_bray <- ordinate(ps, "NMDS", "bray")
plot_ordination(ps, ps_bray, type="samples", color="diet_group", shape = "sex") + geom_point(size = 3) 

```








*Step 3: Taxon cleaning*
The following R chunk removes taxa not-typically part of a bacterial microbiome analysis.

```{r taxon-cleaning}
# Some examples of taxa you may not want to include in your analysis
get_taxa_unique(ps1, "Kingdom")
get_taxa_unique(ps1, "Class")
ps1 # Check the number of taxa prior to removal
ps2 <- ps1 %>%
  subset_taxa(
    Kingdom == "Bacteria" #&
    #Family  != "mitochondria" &
    #Class   != "Chloroplast" &
    #Phylum != "Cyanobacteria/Chloroplast"
)
ps2 # Confirm that the taxa were removed
get_taxa_unique(ps1, "Kingdom")
get_taxa_unique(ps2, "Kingdom")

get_taxa_unique(ps1, "Class")
get_taxa_unique(ps2, "Class")

```

## Prevalance assessment

Identification of taxa that are poorly represented in an unsupervised manner can identify taxa that will have little to no effect on downstream analysis. Sufficient removal of these "low prevalence" features can enhance many analysis by focusing statistical testing on taxa common throughout the data.

This approach is frequently poorly documented or justified in methods sections of manuscripts, but will typically read something like, "Taxa present in fewer than 3% of all of the samples within the study population and less than 5% relative abundance were removed from all subsequent analysis.".

While the ultimate selection criteria can still be subjective, the following plots can be useful for making your selection criteria.

```{r prevalence-assessment}
# Prevalence estimation
# Calculate feature prevalence across the data set
prevdf <- apply(X = otu_table(ps2),MARGIN = ifelse(taxa_are_rows(ps2), yes = 1, no = 2),FUN = function(x){sum(x > 0)})
# Add taxonomy and total read counts to prevdf
prevdf <- data.frame(Prevalence = prevdf, TotalAbundance = taxa_sums(ps2), tax_table(ps2))
#Prevalence plot
prevdf1 <- subset(prevdf, Phylum %in% get_taxa_unique(ps0, "Phylum"))
p.prevdf1 <- ggplot(prevdf1, aes(TotalAbundance, Prevalence / nsamples(ps2),color=Family)) +
  geom_hline(yintercept = 0.01, alpha = 0.5, linetype = 2) +
  geom_point(size = .5, alpha = 0.7) +
  scale_x_log10() +
  xlab("Total Abundance") + ylab("Prevalence [Frac. Samples]") +
  facet_wrap(~Phylum) +
  theme(legend.position="none") +
  ggtitle("Phylum Prevalence in All Samples\nColored by Family")
p.prevdf1
```
This code will produce a plot of all of the Phyla present in your samples along with information about their prevalence (fraction of samples they are present in) and total abundance across all samples. In this example we drew a dashed horizontal line to cross at the 5% prevalence level (present in > 5% of all of the samples in the study). If you set a threshold to remove taxa below that level you can visually see how many and what types of taxa will be removed. Whatever, threshold you choose to use it should be well documented within your materials and methods.


An example on how to filter low prevalent taxa is below.

```{r prevelance-filtering}
# Remove specific taxa
# Define a list with taxa to remove
#filterPhyla = c("Fusobacteria", "Tenericutes")
#filterPhyla
#get_taxa_unique(ps2, "Phylum")
#ps2.prev <- subset_taxa(ps2, !Phylum %in% filterPhyla) 
#get_taxa_unique(ps2.prev, "Phylum")
# Removing taxa that fall below 5% prevelance
# Define the prevalence threshold
prevalenceThreshold = 0.01 * nsamples(ps2)
prevalenceThreshold
# Define which taxa fall within the prevalence threshold
keepTaxa <- rownames(prevdf1)[(prevdf1$Prevalence >= prevalenceThreshold)]
ntaxa(ps2)
# Remove those taxa
ps2.prev <- prune_taxa(keepTaxa, ps2)
ntaxa(ps2.prev)
```

Because  agglomerate taxa at the Genus level (combine all with the same name) and remove all taxa without genus level assignment

```{r}
#We can do it using a taxonomic method 

# How many genera would be present after filtering?
length(get_taxa_unique(ps2, taxonomic.rank = "Genus"))
## [1] 49
ps3 = tax_glom(ps2.prev, "Genus", NArm = TRUE)

sum(colSums(otu_table(ps3)))
```

```{r}
#Or by determining a tree hight at which you want to conglomerate the measures 

h1 = 0.4
ps4 = tip_glom(ps2.prev, h = h1)
sum(colSums(otu_table(ps4)))

```

We can generate a tree picture from each method and compare it to the previous tree "Before Agglomeration"

```{r}
multiPlotTitleTextSize = 15
p2tree = plot_tree(ps2, method = "treeonly",
                   ladderize = "left",
                   title = "Before Agglomeration") +
  theme(plot.title = element_text(size = multiPlotTitleTextSize))
p3tree = plot_tree(ps3, method = "treeonly",
                   ladderize = "left", title = "By Genus") +
  theme(plot.title = element_text(size = multiPlotTitleTextSize))
p4tree = plot_tree(ps4, method = "treeonly",
                   ladderize = "left", title = "By Height") +
  theme(plot.title = element_text(size = multiPlotTitleTextSize))
# group plots together
grid.arrange(nrow = 1, p2tree, p3tree, p4tree)
```

It is usually necessary to transform microbiome count data to account for differences in library size, variance, scale, etc. The phyloseq package provides a flexible interface for defining new functions to accomplish these transformations of the abundance values via the function transform_sample_counts(). The first argument to this function is the phyloseq object you want to transform, and the second argument is an R function that defines the transformation. The R function is applied sample-wise, expecting that the first unnamed argument is a vector of taxa counts in the same order as the phyloseq object. Additional arguments are passed on to the function specified in the second argument, providing an explicit means to include pre-computed values, previously defined parameters/thresholds, or any other object that might be appropriate for computing the transformed values of interest.

This example begins by defining a custom plot function, plot_abundance(), that uses phyloseq’s function to define a relative abundance graphic. We will use this to compare more easily differences in scale and distribution of the abundance values in our phyloseq object before and after transformation.
```{r}
plot_abundance = function(physeq,title = "",
                          Facet = "Order", Color = "sex"){
  # Arbitrary subset, based on Phylum, for plotting
  p1f = subset_taxa(ps2.prev, Phylum %in% c("Firmicutes"))
  mphyseq = psmelt(p1f)
  mphyseq <- subset(mphyseq, Abundance > 0)
  ggplot(data = mphyseq, mapping = aes_string(x = "sex",y = "Abundance",
                              color = Color, fill = Color)) +
    geom_violin(fill = NA) +
    geom_point(size = 1, alpha = 0.3,
               position = position_jitter(width = 0.3)) +
    facet_wrap(facets = Facet) + scale_y_log10()+
    theme(legend.position="none")
}
```
The transformation in this case converts the counts from each sample into their frequencies, often referred to as proportions or relative abundances. This function is so simple that it is easiest to define it within the function call to transform_sample_counts().
```{r}
# Transform to relative abundance. Save as new object.
ps2ra = transform_sample_counts(ps2.prev, function(x){x / sum(x)})
```
Now we plot the abundance values before and after transformation.

```{r}
plotBefore = plot_abundance(ps2,"")
plotAfter = plot_abundance(ps2ra,"")
# Combine each plot into one graphic.
grid.arrange(nrow = 2,  plotBefore, plotAfter)
```
```{r}
psOrd = subset_taxa(ps2ra, Order == "Bacillales")
plot_abundance(psOrd, Facet = "Family", Color = NULL)
```

Do some simple ordination looking for outlier samples, first we variance stabilize the data with a log transform, the perform PCoA using bray’s distances

```{r}
logt  = transform_sample_counts(ps3, function(x) log(1 + x) )
out.pcoa.logt <- ordinate(logt, method = "PCoA", distance = "bray")
evals <- out.pcoa.logt$values$Eigenvalues
plot_ordination(logt, out.pcoa.logt, type = "samples", 
                color = "diet_group", shape = "sex") + labs(col = "zt_time") +
  coord_fixed(sqrt(evals[2] / evals[1]))

```

To generate a heatmaap 

```{r}
 plot_heatmap(ps3, method = "NMDS", distance = "bray")
```
its too cluttered so we should only count the most abundant AVSs

```{r}
 total = median(sample_sums(ps3))
  ps3_abund <- filter_taxa(ps3, function(x) sum(x > total*0.20) > 0, TRUE)
  ps3_abund
  
    plot_heatmap(ps3_abund, method = "NMDS", distance = "bray")

```




```{r}
.cran_packages <- c( "shiny","miniUI", "caret", "pls", "e1071", "ggplot2", "randomForest", "dplyr", "ggrepel", "nlme", "devtools",
                  "reshape2", "PMA", "structSSI", "ade4",
                  "ggnetwork", "intergraph", "scales")
.github_packages <- c("jfukuyama/phyloseqGraphTest")
.bioc_packages <- c("genefilter", "impute")
# Install CRAN packages (if not already installed)
.inst <- .cran_packages %in% installed.packages()
if (any(!.inst)){
  install.packages(.cran_packages[!.inst],repos = "http://cran.rstudio.com/")
}
.inst <- .github_packages %in% installed.packages()
if (any(!.inst)){
  devtools::install_github(.github_packages[!.inst])
}

.inst <- .bioc_packages %in% installed.packages()
if(any(!.inst)){
  source("http://bioconductor.org/biocLite.R")
  biocLite(.bioc_packages[!.inst])
}
```



