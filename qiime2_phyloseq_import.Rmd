---
title: "qiime2_phyloseq_import"
author: "Diana_Gutierrez"
date: "8/20/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## R Markdown

This is an R Markdown document. Markdown is a simple formatting syntax for authoring HTML, PDF, and MS Word documents. For more details on using R Markdown see <http://rmarkdown.rstudio.com>.

When you click the **Knit** button a document will be generated that includes both content as well as the output of any embedded R code chunks within the document. You can embed an R code chunk like this:
To load the Bioconductor package follow the instructions in the following website: 
https://cran.r-project.org/web/packages/BiocManager/vignettes/BiocManager.html

##Environment inititation

We will begin by customizing our global settings, activating packages and loading our data into R using the following steps:

1) Set global knitr options
2) Load libraries (the app store of R)
3) Set global ggplot2 theme and options
4) Load data


### Set global knitr options

Knitr is a standardized library which "knits" together code chunks and converts them to specified format such as HTML or PDF. This is very useful for report generation. The way in which knitr handles chunk formatting and report generation can be specified in a code chunk. 

There are a large number of ways to customize R code chunks. For the knitr and ggplot2 theme settings (below) I have decided to set include=FALSE (e.g. {r global_options, include=FALSE}). This tells knitr to exclude the chunk from the final report. In this case, the chunk will still be evaluated as part of the RMarkdown document. If you wish to prevent the chunk from being executed at all you can set eval=FALSE.There are a number options you can use in this section [read about here](https://yihui.name/knitr/options/).

```{r global_options, include=FALSE}
# This chunk defines output figure dimensions,
# specifies a path where knitted figures will reside after knitting, 
# and prevents display of warnings in the knitted report
knitr::opts_chunk$set(fig.width=8,
                      fig.height=6,
                      fig.path="../figures/",
                      dev='png',
                      warning=FALSE,
                      message=FALSE)
```


You have to install BiocManager to be able to install the libraries that qiime2R depends on
```{r}
chooseCRANmirror()
install.packages("BiocManager")

#To install dependencies from the BiocManager you can use the following command: 

BiocManager::install(c("Biostrings", "biomformat", "phyloseq"))
BiocManager::install(c("DESeq2"))
BiocManager::install(c("microbiome"))
```


To install qiime2R you can reffer to this tutorial
https://forum.qiime2.org/t/tutorial-integrating-qiime2-and-r-for-data-visualization-and-analysis-using-qiime2r/4121


or use the following command:
if (!requireNamespace("devtools", quietly = TRUE)){install.packages("devtools")}
devtools::install_github("jbisanz/qiime2R") # current version is 0.99.20

## Load libraries

```{r initiate-environment}
# Each line below will load a R library and print its currently installed version
# Notes may be displayed as the packages load, but for the most part these can be ignored
install.packages("ape")
install.packages("Hmisc")
installed.packages("yaml")
install.packages("tidyr")
installed.packages("dplyr")
install.packages("DESeq2")

library("plyr"); packageVersion("plyr")
library("tidyverse"); packageVersion("tidyverse")
library("phyloseq"); packageVersion("phyloseq")
library("vegan"); packageVersion("vegan")
library("gridExtra"); packageVersion("gridExtra")
library("knitr"); packageVersion("knitr")
library("DESeq2"); packageVersion("DESeq2")
library("plotly"); packageVersion("plotly")
library("microbiome"); packageVersion("microbiome")
library("ggpubr"); packageVersion("ggpubr")
library("data.table"); packageVersion("data.table")

#All other dependencies are included in the following libraries 


library(knitr)
library(qiime2R)
library(ape)
library(Biostrings)
library(biomformat)
library(phyloseq)
library(cowplot)
library(Hmisc)
library(yaml)
library(tidyr)
library(dplyr)

```

## Set global ggplot2 theme and options

This sets the plotting aesthetics for every ggplot2 for the rest of the document. There are a tremendous number of ways to customize your ggplot2 settings using theme_set (see: http://ggplot2.tidyverse.org/reference/theme_get.html). It is best practice to do this at the beginning of the RMarkdown document so that these settings propagated to every plot through the rest of the document.

```{r global-theme-settings, include=FALSE}
# Set global theming
# This theme set will change the ggplot2 defaults to use the b&w settings (removes the default gray background) and sets the default font to 12pt Arial
theme_set(theme_bw(base_size = 12))
```
## Read in your data

Since we are working from running the DADA2 program on our samples in Qiime, first we load the table.gza object to the read_qza function to convert it into a phyloseq object 

```{r}
#I am setting the working directory to the location of the table.qza object I want to transform

list.files()
#To see details on how the main functions stores the qiime2 artifact you can run: 
?read_qza

#Details on how the data is read into the object can be ween in the help window on your  right --->

SVs<-read_qza("q2_artfcts/table.qza")

names(SVs)

#read the metadata 

metadata<-read_q2metadata("metadata/metadata_cat.tsv")
head(metadata)
type(metadata)
taxonomy<-read_qza("q2_artfcts/taxonomy.qza")
head(taxonomy$data)

#When the taxonomy artifact is imported it is imported as a string so we need to parse it to a table so we can use in downstream analysis

taxonomy<-parse_taxonomy(taxonomy$data)
head(taxonomy)

```


##Generatoin of the phyloseq object 

Now we generate the phyloseq object, by reading each one of the artifacts exported from qiime using the qza_to_phyloseq function. Note that the metada file has to have the file format including #q2:types comments in the second row of the file in order for the function to work properly and it does not accept missing values in the file. 

```{r}

ps<-qza_to_phyloseq(
    features="q2_artfcts/table.qza",
    tree="q2_artfcts/rooted-tree.qza","q2_artfcts/taxonomy.qza",
    metadata = "metadata/metadata_cat.tsv"
    )
sample_variables(ps) #Display variables from the mapping file
sample_data(ps)
ntaxa(ps) #Total number of taxa in the entire data 
rank_names(ps) #Taxonomic ranks = 6738
get_taxa_unique(ps, "Phylum")
summary(taxa_sums(ps))
table(tax_table(ps) [, "Phylum"], exclude=NULL)
#get_taxa_unique(ps, "Class")
#get_taxa_unique(ps, "Order")
#get_taxa_unique(ps, "Family")
#get_taxa_unique(ps, "Species")

```
The phyla where only one feature was observed, may be worth filtering, in this case they include Crenarcheota, SR1, Chlamydiae, Synergistetes, WPS-2, Chlorobi, GN02. The phyla that are ambiguously identified "<NA>" may need to be eliminated. There is also contamination from non bacterial phyla like Chloroflexi and we should eliminate those as well.  


##Factor reordering and renaming

The default sorting for ggplot2 is alphabetical. For example, if you want to make a box plot comparing Shannon diversity between MLF and MHF mice, it will by default always place knockout on the MHF and MLF on the right. However, you may wish to switch this order.

This can be done on a plot-by-plot basis, however, it is likely that you will want all of your plots to reflect this customization throughout 
the entire analysis, so it is useful to have an R chunk at the very beginning of your workflow to specify order and label names.

In the example data, most of the analysis will be done comparing the sample variable "diet_group" which is either MHF, MLF, EHF or ELF included in the mapping file. 

```{r factor-adjustments}
# Reorder Diet Groups
levels(sample_data(ps0)$diet_group)
sample_data(ps0)$diet_group <- factor(sample_data(ps0)$diet_group, levels = c("MHF","MLF","EHF","LHF"))
levels(sample_data(ps0)$diet_group)

# Reorder Time points
levels(sample_data(ps0)$zt_time)
sample_data(ps0)$zt_time <- factor(sample_data(ps0)$zt_time, levels = c("0", "4", "8", "12", "16", "20"))
levels(sample_data(ps0)$zt_time)


```

##Filtering samples 

#Eliminate samples from different timepoints
Our samples include several weeks of the study and we may want to filter out samples by an specific variable, in this case we will only analyse the data at week 13 of the study so we will filter all samples where the variable week_name is "Baseline"

```{r}
levels(sample_data(ps)$week_name)
ps<-subset_samples(ps, week_name != "Baseline")
levels(sample_data(ps)$week_name)

#We also want to exclude from the analysis any animals that had irregular behavior and we have already excluded from the statistical analysis.

levels(sample_data(ps)$animal_id)
ps<-subset_samples(ps, animal_id != "13")
levels(sample_data(ps)$animal_id)

```

Removed taxa no longer part of the count table due to sample removal

```{r}
summary(taxa_sums(ps))
```

```{r}
ps<-prune_taxa(taxa_sums(ps)>0, ps)
summary(taxa_sums(ps))
```

```{r}
table(tax_table(ps) [, "Phylum"], exclude=NULL)
```
Now we can remove samples that have ambiguous phylum annotation 

```{r}
ps0<-subset_taxa(ps, !is.na(Phylum) & !Phylum %in% c("", "uncharacterized")
)
summary(taxa_sums(ps0))
table(tax_table(ps0) [, "Phylum"], exclude=NULL)
```
##Investigating low prevalence/Abundance phylum and subset them out 

```{r}
#First we generate a prevalence table
prevelancedf = apply(X = otu_table(ps0),
                 MARGIN = 1,
                 FUN = function(x){sum(x > 0)})
# Add taxonomy and total read counts to this data.frame
prevelancedf = data.frame(Prevalence = prevelancedf,
                      TotalAbundance = taxa_sums(ps0),
                      tax_table(ps0))
prevelancedf[1:10,]


plyr::ddply(prevelancedf, "Phylum", function(df1){
  data.frame(mean_prevalence=mean(df1$Prevalence),sum_prevalence=sum(df1$Prevalence),total_abundance=sum(df1$TotalAbundance,na.rm = T),stringsAsFactors = F)})
```


```{r}


# Compute prevalence of each feature, store as data.frame
prevdf = apply(X = otu_table(ps0),
                 MARGIN = ifelse(taxa_are_rows(ps0), yes = 1, no = 2),
                 FUN = function(x){sum(x > 0)})
# Add taxonomy and total read counts to this data.frame
prevdf = data.frame(Prevalence = prevdf,
                      TotalAbundance = taxa_sums(ps0),
                      tax_table(ps0))

plyr::ddply(prevdf, "Phylum", function(df1){cbind(mean(df1$Prevalence),sum(df1$Prevalence))})
```
```{r}

ps0


```


#We will filter the samples with very low frequency reads (>5) 

```{r}
phyla2filter = c("Acidobacteria", "Armatimonadetes", "Chlamydiae", "Gemmatimonadetes", "GN02", "Lentisphaerae", "OD1","Planctomycetes")

#Filter entries with phylums included in the list we made
ps0=subset_taxa(ps0, !Phylum %in% phyla2filter)

ps0

```


## Data summary and assessment

While there are numerous possible ways to evaluate your data, a standard starting approach would consist of the following steps:

1) Evaluate Amplicon Sequence Variants (ASV) summary statistics
2) Detect and remove outlier samples
3) Taxon cleaning
4) Prevalence estimation and filtering

*Step 1: Evaluate Amplicon Sequence Variants (ASV) summary statistics*
Begin by running the following R chunk to produce ASV summary plots

```{r data-assessment}
# Create a new data frame of the sorted row sums, a column of sorted values from 1 to the total number of individuals/counts for each ASV and a categorical variable stating these are all ASVs.
readsumsdf <- data.frame(nreads = sort(taxa_sums(ps0), decreasing = TRUE), 
                        sorted = 1:ntaxa(ps0),
                        type = "ASVs")
# Make a data frame with a column for the read counts of each sample for histogram production
sample_sum_df <- data.frame(sum = sample_sums(ps0))
# Make plots
# Generates a bar plot with # of reads (y-axis) for each taxa. Sorted from most to least abundant
# Generates a second bar plot with # of reads (y-axis) per sample. Sorted from most to least
p.reads = ggplot(readsumsdf, aes(x = sorted, y = nreads)) +
  geom_bar(stat = "identity") +
  ggtitle("ASV Assessment") +
  scale_y_log10() +
  facet_wrap(~type, scales = "free") +
  ylab("# of Sequences")
# Histogram of the number of Samples (y-axis) at various read depths
p.reads.hist <- ggplot(sample_sum_df, aes(x = sum)) + 
  geom_histogram(color = "black", fill = "firebrick3", binwidth = 150) +
  ggtitle("Distribution of sample sequencing depth") + 
  xlab("Read counts") +
  ylab("# of Samples")
# Final plot, side-by-side
grid.arrange(p.reads, ncol = 1)
grid.arrange( p.reads.hist, ncol=1)
# Basic summary statistics
summary(sample_sums(ps0))
```

The above data assessment is useful for getting an idea of 
1) the number of sequences per taxa (first plot). This will normally be a "long tail" with some taxa being highly abundant in the data tapering off to taxa with very few reads, 

2) the number of reads per sample (second table). Note the spike at the lowest number of reads due to samples taken from mice given antibiotics. Very low read count can be indicative of a failed reaction. Both of these plots will help give an understanding of how your data are structured across taxa and samples and will vary depending on the nature of your samples.

Samples with unexpectedly low number of sequences can be considered for removal. This is an intuitive process and should be instructed by your understanding of the samples in your study. For example, if you have 5 samples from stool samples, one would expect to obtain thousands, if not several thousands of ASV. This may not be the case for other tissues, such as spinal fluid or tissue samples. Similarly, you may not expect thousands of ASV from samples obtained from antibiotic treated organisms. Following antibiotic treatment you may be left with only dozens or hundreds of ASV. So contextual awareness about the biology of your system should guide your decision to remove samples based on ASV number.

Importantly, at each stage you should document and justify your decisions. If you are concerned that sample removal will alter the interpretation of your results, you should run your analysis on the full data and the data with the sample(s) removed to see how the decision affects your interpretation.

The above plots provide overall summaries about the number of ASV found in all of your samples. However, they are not very useful for identifying and removing specific samples. One way to do this is using code from the following R chunk.

*Step 2: Detect and remove outlier samples*
Detecting and potentially removing samples outliers (those samples with underlying data that do not conform to experimental or biological expectations) can be useful for minimizing technical variance. One way to identify sample outliers is shown in the R chunk below.

```{r sample-removal-identification}
# Format a data table to combine sample summary data with sample variable data
ss <- sample_sums(ps0)
head(ss)
sd <- as.data.frame(sample_data(ps0))
head(sd)
ss.df <- merge(sd, data.frame("ASV" = ss), by ="row.names")
head(ss.df)
# Plot the data by the treatment variable
y = 30000 # Set a threshold for the minimum number of acceptable reads. Can start as a guess
x = "sex" # Set the x-axis variable you want to examine
label = "animal_id" # This is the label you want to overlay on the points
p.ss.boxplot <- ggplot(ss.df, aes_string(x, y = "ASV", color = "sex")) + 
  geom_boxplot(outlier.colour="NA", position = position_dodge(width = 0.8)) +
  geom_jitter(size = 2, alpha = 0.6) +
  scale_y_log10() +
  facet_wrap(~zt_time) +
  geom_hline(yintercept = y, lty = 2) +
  geom_text(aes_string(label = label), size = 5, nudge_y = 0.05, nudge_x = 0.05)
p.ss.boxplot
```
The example data does hot have many samples with fewer than 3,000 ASV other than the sample 5 in the ZT 12 graph. When questionable samples arise you should take note of them so if there are samples which behave oddly in downstream analysis you can recall this information and perhaps justify their removal. In this case lets remove them for practice. 

```{r sample-outlier-removal}
nsamples(ps0)

levels(sample_data(ps0))
ps1 <- subset_samples(ps0, sample !="5x3x12")

ps1 <- ps0 %>% subset_samples(
    name != "5x3x12"
    )
nsamples(ps1)
```

Note that we created a new PhyloSeq object called ps1. This preserves all of the data in the original ps0 and creates a new data object with the offending samples removed called ps1.

Failure to detect and remove "bad" samples can make interpreting ordinations much more challenging as they typically project as "outliers" severely skewing the rest of the samples. These samples also increase variance and will impede your ability to identify diferentially abundant taxa between groups. Thus sample outlier removal should be a serious and thoughtful part of every analysis in order to obtain optimal results.

*Step 3: Taxon cleaning*
The following R chunk removes taxa not-typically part of a bacterial microbiome analysis.

```{r taxon-cleaning}
# Some examples of taxa you may not want to include in your analysis
get_taxa_unique(ps1, "Kingdom")
get_taxa_unique(ps1, "Class")
ps1 # Check the number of taxa prior to removal
ps2 <- ps1 %>%
  subset_taxa(
    Kingdom == "Bacteria" &
    Family  != "mitochondria" &
    Class   != "Chloroplast" &
    Phylum != "Cyanobacteria/Chloroplast"
  )
ps2 # Confirm that the taxa were removed
get_taxa_unique(ps2, "Kingdom")
get_taxa_unique(ps2, "Class")
```

## Prevalance assessment

Identification of taxa that are poorly represented in an unsupervised manner can identify taxa that will have little to no effect on downstream analysis. Sufficient removal of these "low prevalence" features can enhance many analysis by focusing statistical testing on taxa common throughout the data.

This approach is frequently poorly documented or justified in methods sections of manuscripts, but will typically read something like, "Taxa present in fewer than 3% of all of the samples within the study population and less than 5% relative abundance were removed from all subsequent analysis.".

While the ultimate selection criteria can still be subjective, the following plots can be useful for making your selection criteria.

```{r prevalence-assessment}
# Prevalence estimation
# Calculate feature prevalence across the data set
prevdf <- apply(X = otu_table(ps2),MARGIN = ifelse(taxa_are_rows(ps2), yes = 1, no = 2),FUN = function(x){sum(x > 0)})
# Add taxonomy and total read counts to prevdf
prevdf <- data.frame(Prevalence = prevdf, TotalAbundance = taxa_sums(ps2), tax_table(ps2))
#Prevalence plot
prevdf1 <- subset(prevdf, Phylum %in% get_taxa_unique(ps0, "Phylum"))
p.prevdf1 <- ggplot(prevdf1, aes(TotalAbundance, Prevalence / nsamples(ps2),color=Family)) +
  geom_hline(yintercept = 0.0005, alpha = 0.5, linetype = 2) +
  geom_point(size = 3, alpha = 0.7) +
  scale_x_log10() +
  xlab("Total Abundance") + ylab("Prevalence [Frac. Samples]") +
  facet_wrap(~Phylum) +
  theme(legend.position="none") +
  ggtitle("Phylum Prevalence in All Samples\nColored by Family")
p.prevdf1
```
This code will produce a plot of all of the Phyla present in your samples along with information about their prevalence (fraction of samples they are present in) and total abundance across all samples. In this example we drew a dashed horizontal line to cross at the 5% prevalence level (present in > 5% of all of the samples in the study). If you set a threshold to remove taxa below that level you can visually see how many and what types of taxa will be removed. Whatever, threshold you choose to use it should be well documented within your materials and methods.


An example on how to filter low prevalent taxa is below.

```{r prevelance-filtering}
# Remove specific taxa
# Define a list with taxa to remove
filterPhyla = c("Fusobacteria", "Tenericutes")
filterPhyla
get_taxa_unique(ps2, "Phylum")
ps2.prev <- subset_taxa(ps2, !Phylum %in% filterPhyla) 
get_taxa_unique(ps2.prev, "Phylum")
# Removing taxa that fall below 5% prevelance
# Define the prevalence threshold
prevalenceThreshold = 0.05 * nsamples(ps2)
prevalenceThreshold
# Define which taxa fall within the prevalence threshold
keepTaxa <- rownames(prevdf1)[(prevdf1$Prevalence >= prevalenceThreshold)]
ntaxa(ps2)
# Remove those taxa
ps2.prev <- prune_taxa(keepTaxa, ps2)
ntaxa(ps2.prev)
```
